# Instructions on running data processing tasks
# Data Processing Scripts (`data_processing/`)

This directory contains Python scripts responsible for offline data processing tasks required by the MovieLens Recommender system. These tasks include downloading the dataset, generating embeddings, and loading initial data into the database.

**Crucially, these scripts are designed to be run *outside* the main backend API service (hosted on Cloud Run).** They are intended for one-time setup or periodic execution via mechanisms like:

*   Manual execution on a local machine or temporary VM.
*   Google Cloud Functions (Gen 2 recommended for potentially longer runtimes/more memory).
*   Google Cloud Workflows orchestrating Cloud Functions or other services.

## Scripts Overview

The scripts should generally be run in the following order:

1.  **`scripts/01_download_movielens.py`**
    *   **Purpose:** Checks if the specified MovieLens dataset zip file (e.g., `ml-latest-small.zip`) exists in the configured GCS bucket. If not, it downloads the zip file from the official MovieLens URL and uploads it to the GCS bucket.
    *   **Input:** Environment variables (`MOVIELENS_URL`, `GCS_BUCKET_NAME`, `MOVIELENS_ZIP_FILENAME`).
    *   **Output:** The dataset zip file stored in GCS. Logs status to console (JSON format).

2.  **`scripts/02_generate_embeddings.py`**
    *   **Purpose:** Reads `movies.csv` from the zip file stored in GCS. For each movie, it prepares a text representation (title + genres) and uses a specified Hugging Face Sentence Transformer model to generate a content embedding vector. It then loads the movie data (including the generated embedding and a newly generated MongoDB `_id`) into the MongoDB `movies` collection. It also creates a mapping file (`movie_id_map.json`) linking the original `movieId` from the CSV to the new MongoDB `_id`.
    *   **Input:** GCS object path (from env vars), MongoDB connection details (from env vars), Hugging Face model name (from env vars).
    *   **Output:** Populated `movies` collection in MongoDB. `movie_id_map.json` file created locally. Logs status to console (JSON format).
    *   **Note:** This script can be memory and time-intensive depending on the dataset size and the chosen embedding model. Run on a machine with sufficient resources or configure Cloud Function with adequate memory/timeout.

3.  **`scripts/03_load_interactions.py`**
    *   **Purpose:** Reads `ratings.csv` from the zip file stored in GCS. It uses the `movie_id_map.json` file (generated by script 02) to map the `movieId` from the ratings file to the corresponding MongoDB `_id` in the `movies` collection. It then formats and loads these interactions (as 'rate' type) into the MongoDB `interactions` collection.
    *   **Input:** GCS object path, MongoDB connection details, `movie_id_map.json` file (must exist).
    *   **Output:** Populated `interactions` collection in MongoDB. Logs status to console (JSON format).

4.  **`scripts/04_update_recommendations.py`** (Optional / Example)
    *   **Purpose:** Performs a simple, periodic update task. This example calculates recently popular movies based on high ratings within a defined timeframe and stores the list of popular movie IDs in Redis (or another cache/DB table) to be used as a fallback recommendation set by the main API.
    *   **Input:** MongoDB connection details, Redis connection details (optional, from env vars), configuration parameters (recent days, min rating, etc.).
    *   **Output:** Updates a specific cache key (e.g., `rec:fallback:popular`) in Redis. Logs status to console (JSON format).

## Prerequisites

*   Python 3.9+ and Pip
*   Access to the configured Google Cloud Storage bucket.
*   Access to the configured MongoDB instance (e.g., Atlas M0).
*   (Optional) Access to the configured Redis instance.
*   Configured Google Cloud Application Default Credentials (ADC) if running locally and accessing GCS (e.g., via `gcloud auth application-default login`).

## Setup

1.  Navigate to this `data_processing/` directory:
    ```bash
    cd path/to/movielens-recommender-gcp/data_processing
    ```
2.  Create and activate a Python virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3.  Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```
4.  Copy the environment variable template:
    ```bash
    cp .env.sample .env
    ```
5.  Edit the `.env` file and fill in your actual configuration values (GCS bucket name, MongoDB URI, Redis URL, etc.). **Do not commit the `.env` file to version control.**

## Configuration

The scripts rely on environment variables for configuration. These can be set directly in your environment or loaded from the `.env` file (using `python-dotenv`) when running locally. Key variables are listed in `.env.sample`.

*   `LOG_LEVEL`: Controls script logging verbosity (e.g., `INFO`, `DEBUG`).
*   `GCS_BUCKET_NAME`: Name of your GCS bucket.
*   `MONGODB_URI`: Connection string for your MongoDB database.
*   `MOVIELENS_URL`: URL to download the dataset zip.
*   `MOVIELENS_ZIP_FILENAME`: Expected name of the zip file in GCS.
*   `HF_MODEL_NAME`: Hugging Face model for embeddings.
*   `HF_DEVICE`: Optional device for embedding ('cuda', 'cpu').
*   `EMBEDDING_BATCH_SIZE`: Batch size for embedding generation.
*   `REDIS_URL`: (Optional) Connection URL for Redis cache.
*   ... (other script-specific variables)

When deploying as Cloud Functions, these environment variables must be configured directly in the function's settings (preferably referencing secrets in Secret Manager).

## Running Locally

Ensure your virtual environment is activated and the `.env` file is correctly configured. Run the scripts sequentially from within the `data_processing/` directory:

```bash
# Make sure venv is active and .env is configured

# 1. Download dataset to GCS (if needed)
python scripts/01_download_movielens.py

# 2. Generate embeddings and load movies to MongoDB
# (This may take a while)
python scripts/02_generate_embeddings.py

# 3. Load interactions to MongoDB
# (Requires movie_id_map.json created by script 02)
python scripts/03_load_interactions.py

# 4. Run optional update script (e.g., calculate popular items)
# python scripts/04_update_recommendations.py